---
title: "Activity Correctness Predictions"
author: "Rachel C"
date: "5/18/2021"
output: html_document
---

```{r setup}
library(caret)
```

First, I download both the training and testing data sets.
```{r}

download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", "pml-testing.csv")

training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

Then, I partition the data into two sets so that I can have a validation test set to compare methods with. In addition, many of the variables aren't useful, so I subset the dataset to just the columns I want to use for prediction. 
```{r cache=TRUE}
inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
train <- training[inTrain,]
val <- training[-inTrain,]

#get the raw values and skip the averages and summary stats
train <- train[,c(2, 8:11, 37:49, 60:68, 84:86, 102, 113:124, 151:160)]

```
I try a bunch of different methods and compare the accuracy with the validation data set. 
The most accurate method is "gbm" which is even more accurate (out-sample) than stacking several methods, probably because the accuracy for other methods is so much lower than gbm. 
```{r cache=TRUE, results=FALSE}
gbm <- train(classe ~ ., method = "gbm", data=train)
lda <- train(classe ~ ., method="lda", data=train)
mda <- train(classe ~ ., method="mda", data=train)
rpart <- train(classe ~ ., method = "rpart", data=train)
```

```{r cache=TRUE}
#try a bunch of different methods to see which is the most accurate for the validation dataset
predgbm <- predict(gbm, val)
inConfgbm <- confusionMatrix(predict(gbm, train), as.factor(train$classe))$overall[1]
outconfgbm <- confusionMatrix(predgbm, as.factor(val$classe))$overall[1]

predlda <- predict(lda, val)
confLda <- confusionMatrix(predlda, as.factor(val$classe))$overall[1]
inconfLda <- confusionMatrix(predict(lda, train), as.factor(train$classe))$overall[1]

predmda <- predict(mda, val)
confMda <- confusionMatrix(predmda, as.factor(val$classe))$overall[[1]]
inconfmda <- confusionMatrix(predict(mda, train), as.factor(train$classe))$overall[1]


predrpart <- predict(rpart, val)
confRpart <- confusionMatrix(predrpart, as.factor(val$classe))$overall[1]
inconfRpart <- confusionMatrix(predict(rpart, train), as.factor(train$classe))$overall[1]

predDF <- data.frame(predgbm, predlda, predmda, predrpart, classe = val$classe)
comboModFit <- train(classe ~ ., data = predDF, method = "rf")
comboPred <- predict(comboModFit, predDF)
inconfCombo <- confusionMatrix(comboPred, as.factor(predDF$classe))$overall[1]

```
For GBM, the accuracy for in and out sample is `r round(inConfgbm, 3)` and `r round(outconfgbm, 3)`. 

For LDA, in sample accuracy is `r round(inconfLda, 3)` and out sample accuracy is `r round(confLda, 3)`.

For MDA, in sample accuracy is `r round(inconfmda, 3)` and out sample accuracy is `r round(confMda, 3)`.

for rpart, in sample accuracy is `r round(inconfRpart, 3)` and out sample accuracy is `r round(confRpart, 3)`.

Using model stacking with all four algorithms, the in sample accuracy is `r round(inconfCombo, 3)`. We don't have the out sample accuracy for the stacked model because we don't have the true labels for the testing set. 

We'll use just GBM on the testing dataset.
```{r}
predict(gbm, testing)
```